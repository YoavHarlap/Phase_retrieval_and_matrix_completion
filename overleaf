\documentclass[12pt, a4paper, twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\setcounter{secnumdepth}{3}
\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage[a4paper, inner=3cm]{geometry}
\usepackage{color, soul}
\usepackage[super]{nth}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{color}
\usepackage{subcaption}

\usepackage{mathtools}

\usepackage{booktabs}
\usepackage[svgnames,table]{xcolor}
\usepackage[tableposition=above]{caption}
\usepackage{pifont}

\newcommand{\rev}[1]{{{#1}}}
\newcommand{\re}[1]{{{#1}}}

\usepackage[hang, flushmargin]{footmisc}

\usepackage[section]{placeins}
\usepackage{float}
\usepackage{setspace}
\DeclareMathOperator{\Unif}{Unif}
\usepackage{tikz}


\usetikzlibrary{backgrounds}
\makeatletter

\tikzset{%
  fancy quotes/.style={
    text width=\fq@width pt,
    align=justify,
    inner sep=1em,
    anchor=north west,
    minimum width=\linewidth,
  },
  fancy quotes width/.initial={.8\linewidth},
  fancy quotes marks/.style={
    scale=8,
    text=white,
    inner sep=0pt,
  },
  fancy quotes opening/.style={
    fancy quotes marks,
  },
  fancy quotes closing/.style={
    fancy quotes marks,
  },
  fancy quotes background/.style={
    show background rectangle,
    inner frame xsep=0pt,
    background rectangle/.style={
      fill=gray!25,
      rounded corners,
    },
  }
}

\newenvironment{fancyquotes}[1][]{%
\noindent
\tikzpicture[fancy quotes background]
\node[fancy quotes opening,anchor=north west] (fq@ul) at (0,0) {``};
\tikz@scan@one@point\pgfutil@firstofone(fq@ul.east)
\pgfmathsetmacro{\fq@width}{\linewidth - 2*\pgf@x}
\node[fancy quotes,#1] (fq@txt) at (fq@ul.north west) \bgroup}
{\egroup;
\node[overlay,fancy quotes closing,anchor=east] at (fq@txt.south east) {''};
\endtikzpicture}

\makeatother

\newcommand*{\BeginNoToc}{%
  \addtocontents{toc}{%
    \edef\protect\SavedTocDepth{\protect\the\protect\value{tocdepth}}%
  }%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{-10}%
  }%
}
\newcommand*{\EndNoToc}{%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{\protect\SavedTocDepth}%
  }%
}

%\usepackage[Lenny]{fncychap}

\raggedbottom
%\DeclareMathOperator{\IDFT}{IDFT}

\usepackage[nottoc]{tocbibind}
\usepackage{fancyhdr}

\pagestyle{fancy}
\newcommand{\fncyfront}{%
\fancyhead[RO]{{\footnotesize \rightmark }}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{\footnotesize {\leftmark }}
\fancyfoot[LE]{\thepage }
\fancyhead[RE, LO]{}
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}
\newcommand{\fncymain}{%
\fancyhead[RO]{{\footnotesize \rightmark}}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{{\footnotesize \leftmark}}
\fancyfoot[LE]{\thepage }
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}

\pagestyle{empty}
\newenvironment{abstract}%
{\cleardoublepage \null \vfill \begin{center}%
\bfseries \abstractname \end{center}}%
{\vfill \null}

\usepackage{sectsty}
\allsectionsfont{\sffamily}

%\usepackage{appendix}

\usepackage[titletoc]{appendix}

\usepackage{natbib}
\usepackage{graphicx}

\usepackage{amsthm}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{algorithm}
\usepackage{algorithmicx}

\usepackage{pdfpages}

\usepackage{hyperref}

\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=black,
	linkcolor=blue,
	urlcolor=blue
}
\makeatletter
\def\blankpagewithline{
  \def\@oddhead{\hrulefill} 
  \def\@oddfoot{} 
  \def\@evenhead{\hrulefill} 
  \def\@evenfoot{} 
}
\makeatother

\let\cleardoublepage\clearpage

%opening
% \title{Phase retrieval and matrix completion through projection-based algorithms}
% \author{Yoav Harlap\\ Under the supervision of Prof. Tamir Bendory}

\begin{document}
\pagestyle{fancy}
\fncyfront
\frontmatter
% \maketitle
\includepdf[pages=-]{english_cover_out.pdf}
\newpage
\thispagestyle{empty}
\mbox{}
\includepdf[pages=-]{english_cover_in.pdf}
\newpage
\thispagestyle{empty}
\mbox{}
\chapter*{Acknowledgments}
Prof. Tamir's dedicated guidance, unwavering support, and inspirational approach have been invaluable throughout this journey. His profound knowledge, patience, and insightful advice greatly contributed to shaping this research and fostering personal and professional development. His availability and support, even during the most challenging moments, are deeply appreciated.


\begin{abstract}
This thesis studies phase retrieval and matrix completion, two foundational problems in signal processing. These problems involve reconstructing vectors or matrices from incomplete or corrupted observations, where we can assume additional constraints on the signal. To approach these challenges, this dissertation explores the use of projection-based algorithms and evaluates their performance under varying conditions. 
We investigate the phase retrieval problem, which focuses on recovering the phase vector from Fourier magnitude data. In this case, we also incorporate realistic constraints, such as sparsity, and examine the effects of noise on performance.
Additionally, this work examines the random phase retrieval problem to gain further insights into algorithmic performance across different setups. The matrix completion problem, on the other hand, addresses the recovery of low-rank matrices from partial data, with an emphasis on understanding how the level of missing information affects algorithmic performance.
This thesis compares the performance of these algorithms under varying constraints, highlighting trade-offs in reliability, computational efficiency, and scalability. Comprehensive numerical experiments provide practical guidance for selecting appropriate algorithms and understanding their behavior in different scenarios.
The findings of this study offer insights and practical tools for researchers tackling similar problems. The code to reproduce the numerical results is available at: ~\url{https://github.com/YoavHarlap/Phase_retrieval_and_matrix_completion}.
\end{abstract}



{
\singlespacing
  \hypersetup{linkcolor=black}
  \tableofcontents
\BeginNoToc
\newpage
\listoffigures
%\newpage
%\listoftables
\EndNoToc
}
\onehalfspacing
\fncymain
\mainmatter
\chapter{Introduction}
\label{ch:intro}
\section{Overview and Motivation for Phase Retrieval}
Phase retrieval is a critical problem in several fields, including optics~\cite{fienup1982phase, shechtman2015phase}, signal processing~\cite{candes2013phaselift, balan2006signal} and crystallography~\cite{millane1990phase, elser2018benchmark}. This process involves reconstructing a signal or image from the magnitude of its Fourier transform when the corresponding phase information is missing or inaccessible. The issue arises because detectors are inherently limited to measuring the intensity (the magnitude squared) of a wave, while the phase information is not captured. However, the phase is essential for accurately reconstructing the spatial structure of an object. Without the phase, even with flawless intensity data, the reconstructed image becomes incomplete or distorted.

To understand the importance of phase in the context of image reconstruction, we first consider how an image can be represented in the Fourier domain. Any spatial image can be expressed as a combination of its Fourier magnitude (\(|F(u, v)|\)) and its Fourier phase (\(\phi(u, v)\)):
\[
F(u, v) = |F(u, v)| e^{i\phi(u, v)}.
\]
Here, \(|F(u, v)|\) describes the energy distribution between the frequency components, while \(\phi(u, v)\) determines the spatial arrangement of these components.

Figure~\ref{fig:randomphase} demonstrates the significance of phase. The left image is the original "Astronaut" image, while the middle panel shows its Fourier magnitude. The right image reconstructs the original image using the true magnitude but replacing the phases with random values. The resulting image becomes visually unrecognizable and appears as noise. This highlights that the magnitude alone is insufficient for reconstruction without the phase, as it does not preserve the spatial structure.

\begin{figure}[h!]
    \centering
    % Image a
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig1_original_image.png}
        \\(a) Original image
    \end{minipage}
    % Image b
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig1_magnitude_spectrum.png}
        \\(b) Fourier magnitude (log scale)
    \end{minipage}
    % Image c
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig1_reconstructed_image.png}
        \\(c) Reconstructed with random phases
    \end{minipage}
    \caption[The role of phase in image reconstruction]{The role of phase in image reconstruction. (a) Original image. (b) Fourier magnitude (log scale). (c) Reconstruction with random phases. The images are sourced from the open-access `scikit-image` Python library \cite{van2014scikit}.}
    \label{fig:randomphase}
\end{figure}



To further illustrate the importance of phase, consider the reconstruction shown in Figure~\ref{fig:phasereplacement}. The leftmost image is the original astronaut image. The middle image is reconstructed by combining the Fourier magnitude of the astronaut with the phase of a second image (coins, shown on the right). The reconstructed image adopts the spatial structure of the coins rather than the astronaut, demonstrating that the phase predominantly dictates the spatial features of the reconstruction.

\begin{figure}[h!]
    \centering
    % Image a
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig2_original_image.png}
        \\(a) Original astronaut image
    \end{minipage}
    % Image b
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig2_phase_replaced_image.png}
        \\(b) Reconstruction (astronaut magnitude, coins phase)
    \end{minipage}
    % Image c
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig2_second_image.png}
        \\(c) Coins image (phase source)
    \end{minipage}
    \caption[Phase replacement experiment]{Phase replacement experiment. (a) Original astronaut image. (b) Reconstruction using astronaut magnitude and coins phase. (c) Coins image (phase source). The images are sourced from the open-access `scikit-image` Python library \cite{van2014scikit}.}
    \label{fig:phasereplacement}
\end{figure}




While phase is crucial in many cases, there are specific scenarios where the magnitude plays a more dominant role. Figure~\ref{fig:gridexample} illustrates this using a grid image. The left panel shows the original grid, which is a periodic, structured pattern. The middle panel is reconstructed using the magnitude of the grid combined with the phase of the coins image. Despite the phase change, the reconstruction retains the grid-like structure, indicating that the magnitude primarily defines the visual characteristics in such periodic cases.

\begin{figure}[h!]
    \centering
    % Image a
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig3_grid_image.png}
        \\(a) Original grid image
    \end{minipage}
    % Image b
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig3_combined_image.png}
        \\(b) Reconstruction (grid magnitude, different phase)
    \end{minipage}
    % Image c
    \begin{minipage}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{intro/fig3_second_image_phase_source.png}
        \\(c) Phase source image
    \end{minipage}
    \caption[Grid example: Magnitude dominance]{Grid example: Magnitude dominance. The images are sourced from the open-access `scikit-image` Python library \cite{van2014scikit}.}
    \label{fig:gridexample}
\end{figure}



These experiments provide intuition into the role of phase in image reconstruction.
In natural images, the phase is critical for preserving the spatial structure.
However, in periodic and structured images, such as the grid, the magnitude can dominate while the phase has a reduced impact.

\clearpage
\section{Mathematical Formulation}
In the mathematical formulation of the problem, we consider a matrix \mbox{$A \in \mathbb{C}^{m\times n}$}, which will be referred to as the sensing matrix and a magnitude vector \mbox{$b \in \mathbb{R}^{m}$}, with the understanding that the elements of $b$ are non-negative. Our task in the phase retrieval problem is to solve the following system of equations successfully:
\begin{equation}
\label{eq:model}
|Ax_0| = b.
\end{equation}
\begin{itemize}
    \item The absolute value is applied to each element of the vector \( v \) individually, such that \[|v| = (|v_1|, |v_2|, \dots, |v_n|)\] where \( |v_i| \) represents the absolute value of each entry in \( v \).

    \item The matrix \( A \) can serve various purposes. Often, \( A \) is the Discrete Fourier Transform (DFT) matrix, where multiplying a vector \( x \) with \( A \) performs the Fourier transform. In this case, we are interested in the scenario where \( m = n \). We will also consider cases where \( A \) is a real random matrix or a complex random matrix, with \( m > n \).
\end{itemize}
In many real-world problems, including ours, we can make assumptions about the solution based on additional information. This additional information consists of constraints that the solution must satisfy.

Alternatively, we can define our problem in the following way, and the reason for doing so will become clearer later in the dissertation (see~\ref{sec:set_projection_sudoku}). We can look for a point in the intersection \( x \in \mathcal{A} \cap \mathcal{B} \), where \( \mathcal{B} \) is the set of all signals that satisfy Equation~\eqref{eq:model} and is defined as:
\begin{equation}
\mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}.
\label{eq:model2}
\end{equation}
The set \( \mathcal{A} \) typically represents additional constraints known about our solution, such as sparsity or finite support.


\section{Challenges and Limitations in Phase Retrieval Algorithms}
Over the past decade, considerable focus has been directed toward the computational and theoretical aspects of the phase retrieval problem. To streamline the intricacies of mathematical analysis, a trend emerged wherein researchers adopted an idealized model—one abstracted from practical applications—where the entries of \(A\) are independently and identically distributed (i.i.d.) according to a normal distribution or other similar statistical frameworks. We shall henceforth designate the endeavor of reconstructing a signal under these conditions as the \emph{random phase retrieval problem}. Among the most prevalent algorithms for addressing this problem are those that rely on the minimization of non-convex loss functions, such as non-convex least squares, through the application of first-order gradient methods (e.g., see  \cite{candes2015phase, chen2017solving, cai2016optimal, wang2017solving}). Importantly, this body of work has yielded robust theoretical guarantees, demonstrating that the non-convex nature of the problem is typically well-behaved when the number of measurements \(m\) significantly exceeds the dimensionality \(n\) of the signal being recovered \cite{sun2018geometric, chen2019gradient}.

Regrettably, it has become evident that the random phase retrieval problem presents significantly less complexity compared to the phase retrieval problem that appears in applications, particularly when \(A\) is a Fourier-type matrix. For most practical phase retrieval applications, algorithms developed for the random phase retrieval framework prove inadequate: the inherent non-convexity of the problem does not exhibit benign behavior, and gradient-based methods often converge to local minima, far from any global solution (see the detailed analysis in \cite{elser2018benchmark}). Consequently, despite extensive contributions to this literature, these methods have made only a marginal impact on real-world applications. In practice, heuristic algorithms dominate, including the hybrid input-output (HIO) \cite{fienup1982phase}, difference map \cite{elser2003phase}, relaxed averaged alternating reflections (RAAR) \cite{luke2004relaxed}, and relaxed reflect reflect (RRR) \cite{elser2017matrix}. Each of these methods may be viewed as an extension of the Douglas-Rachford algorithm \cite{douglas1956numerical}. While these algorithms demonstrate strong empirical performance, their properties in the context of the non-convex phase retrieval problem remain largely unexplored.
\clearpage
Several key challenges continue to hinder theoretical advancements in this domain, including:
\begin{itemize}
    \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{candes2015phase}.
    \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmark}.
    \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{li2017relaxed}.
     \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{sun2018geometric, chen2019gradient}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions. They tend to converge rapidly to suboptimal local minima, rather than reaching a solution point.
\end{itemize}

\section{Matrix Completion}
Similar to phase retrieval, matrix completion also aims to reconstruct data from incomplete information but faces unique challenges. Phase retrieval recovers phase from magnitude data, while matrix completion fills in missing entries. The algorithms we will discuss are designed to address both problems.
A common example of matrix completion is the Sudoku puzzle, as illustrated in Figure \ref{fig:sudokuExample}.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_1}
    \caption{Original Sudoku puzzle}
    \label{fig:sudokuOriginal}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_2}
    \caption{Elements to be reconstructed (in blue)}
    \label{fig:sudokuMissing}
  \end{subfigure}
  \caption[An example of a matrix-completion problem is illustrated in Sudoku.]{An example of a matrix completion problem is the Sudoku game. We are given the initial clues (the elements we know) and need to reconstruct the missing elements (the cells painted in blue).}
  \label{fig:sudokuExample}
\end{figure}


Matrix completion has broad applications across different fields:

\begin{itemize}
    \item \textbf{Recommender Systems:} A widely recognized application of matrix completion is collaborative filtering, commonly used in recommender systems, where users typically rate only a small fraction of available items. The missing ratings can be inferred based on the observed data and used to recommend items users are likely to enjoy (e.g., products, movies - see Netflix Prize Model in~\ref{table:Netflix_table}) 
    \cite{candes2012exact}.
    
    \item \textbf{Image Processing:} In image inpainting, matrix completion helps to recover missing or corrupted pixels by treating the image as a matrix. Often, images exhibit a low-rank structure, which allows for effective restoration by completing the missing matrix entries based on this low-rank property \cite{mazumder2010spectral}.
    
    \item \textbf{System Identification:} In fields like control systems, missing data due to sensor faults or communication errors can be problematic. Matrix completion can reconstruct incomplete measurement data, enabling more reliable system identification and control \cite{keshavan2010matrix}.
\end{itemize}


\begin{figure}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
 & \textbf{The Lion King} & \textbf{Avatar} & \textbf{Inception} & \textbf{Titanic} & \textbf{The Avengers} \\
\hline
\textbf{John} & 99\% & 85\% & 80\% & - & 75\% \\
\hline
\textbf{Emily} & - & 10\% & - & 5\% & 77\% \\
\hline
\textbf{Michael} & 99\% & 87\% & 12\% & 90\% & 80\% \\
\hline
\textbf{Sarah} & 91\% & - & 1\% & 97\% & - \\
\hline
\end{tabular}
\caption[Netflix Prize Model]{The Netflix Matrix Model, inspired by the Netflix Prize competition, predicts missing elements in a user-movie matrix based on algorithms. The Netflix Prize aimed to enhance movie recommendations by predicting user ratings from partial data. Key details, including data structure, clustering methods, and \textbf{matrix rank}, ensure the feasibility of this approach.}
\label{table:Netflix_table}
\end{figure}



The foundational approach to matrix completion relies on low-rank matrix recovery. This method assumes that the complete matrix \( M \in \mathbb{R}^{m \times n} \) has a low rank \( r \), where \( r \ll \min(m, n) \). This low-rank condition implies that the matrix has inherent redundancy, allowing the observed data to be effectively represented with fewer components than the total number of entries.

Key reasons for using low-rank matrix completion:

\begin{itemize}
    \item \textbf{Data Reduction and Compact Representation:} Low-rank matrices allow for a compact representation of data, which reduces the number of parameters required to describe it. This property is crucial in matrix completion, as it implies that the observed entries contain enough information to reconstruct the entire matrix, even if many entries are missing.

    \item \textbf{Patterns and Redundancy:} Many types of real-world data exhibit low-rank structures, making low-rank assumptions both practical and meaningful. For instance, in recommender systems, users' preferences often cluster, resulting in a user-item matrix that is typically low-rank. Similarly, in images, spatial coherence means that neighboring pixels tend to have strong correlations. This correlation introduces redundancy, which makes images especially suitable for low-rank approximations in matrix completion tasks.


    \item \textbf{Theoretical Guarantees for Recovery:} With sufficient samples and under specific conditions (such as incoherence and random sampling), low-rank matrix completion can recover the missing entries with high probability \cite{candes2012exact}. This means that, theoretically, a low-rank matrix can be exactly reconstructed from a subset of its entries, which is not generally possible for higher-rank matrices.

    \item \textbf{Computational Efficiency:} Exploiting the low-rank structure leads to efficient algorithms for matrix completion. For example, nuclear norm minimization, which is a convex relaxation of rank minimization, allows the use of efficient optimization techniques, making matrix completion practical for large-scale datasets \cite{mazumder2010spectral}.
\end{itemize}

\section{Research Scope and Objectives}
This thesis seeks to assess the performance of projection-based algorithms across diverse scenarios, with an emphasis on highlighting their strengths and limitations in various contexts. The evaluation is centered on three principal cases:  

\begin{enumerate}
    \item Random phase retrieval problem: The matrix \( A \) in the model defined in\(~\eqref{eq:model}\) is a random matrix, where every element is drawn i.i.d. from a normal distribution, and it can be either real or complex. 

    \item Crystallographic phase retrieval problem: This involves recovering the missing phase information from Fourier magnitude samples, where the matrix \( A \) represents the DFT matrix. As explained in \cite{elser2018benchmark}, we focus on reconstructing a sparse signal, with sparsity serving as a constraint on the signal.

    \item Matrix completion problem: In this case, one constraint ensures that the known initial elements are imposed, and the second constraint typically ensures that the rank of the matrix is fixed according to prior knowledge.
\end{enumerate}
  
\chapter{Projection on Sets Method}
\label{chap:projectionSetsMethod}


\section{Detailed Projections for Case Studies}
This section provides an overview of the operations used to manage constraints in our problem. Each constraint is represented as a projection onto a defined space, ensuring the signal resides within the space where the constraint is satisfied. We start by defining the constraints through their projections and explaining their role in the solution process. Later, we will demonstrate how the algorithms manage the interaction between two projections.

The solution to our problem is to find the intersection point \( x_0 \in \mathcal{A} \cap \mathcal{B} \). To achieve this, the algorithms presented require an explicit definition of each constraint and the ability to compute the projections efficiently.
Let \( y \in \mathbb{C}^n \) be a vector in \( n \)-dimensional complex space. We define the projection of \( y \) onto a set \( \mathcal{A} \) as \( P_\mathcal{A}(y) \), and the projection onto a set \( \mathcal{B} \) as \( P_\mathcal{B}(y) \). A solution is a point where the projections onto both sets $\mathcal{A}$ and $\mathcal{B}$ coincide, meaning that the projections of the point are in the intersection $\mathcal{A} \cap \mathcal{B}$.

\begin{definition}
A point \( y_0 \in \mathbb{C}^m \) is considered a solution if it satisfies the condition \( P_{\mathcal{A}}(y_0) = P_{\mathcal{B}}(y_0) \).
\end{definition} 
\noindent This work focuses on problems when exact solutions exist.
For a general \( x \in \mathbb{C}^n \), let \( y = Ax \in \mathbb{C}^m \). We focus on projectors in terms of \( y \) rather than \( x \) because computing the projector onto \( \mathcal{B} \) is significantly less expensive \cite{li2017relaxed, levin2019note}.
In the random phase retrieval problem and the crystallographic phase retrieval problem, the set \( \mathcal{B} \), as defined in \eqref{eq:model2}, is given by:
\[
\mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}.
\]
The projector of \( y \) onto the set \( \mathcal{B} \)  is defined as

\[
P_{\mathcal{B}}(y) = b \odot \text{phase}(y),
\]
where \( b \) represents the measured magnitudes (see equation ~\eqref{eq:model}), \( \odot \) denotes the point-wise product, and the phase operator is defined element-wise as

\[
\text{phase}(y)[i] := \frac{y[i]}{|y[i]|}, \quad y[i] \neq 0,
\]
and zero otherwise.

In the random phase retrieval problem, the entries of the sensing matrix are typically drawn i.i.d. from a normal distribution with \( m > 2n \). To represent a solution, the point \( y_0 \in \mathbb{C}^m \) must lie within the column space of the matrix \( A \), meaning \( y_0 = AA^\dagger y_0 \), where \( A^\dagger \) denotes the pseudo-inverse of \( A \). The column space of \( A \), also known as the image of \( A \), includes all vectors that can be expressed as \( A x \) for some vector \( x \). 
Thus, for any vector \( y \in \mathbb{C}^m \) to be a valid solution, it must be in the image of \( A \), meaning there exists a vector \( x \) such that \( y = A x \). Specifically, the projection of \( y \) onto the column space of \( A \) is given by:

\[
P_{\mathcal{A}}(y) = A A^\dagger y.
\]
It has been shown (see, for example, \cite{balan2006signal, bandeira2014saving, conca2015algebraic, eldar2014phase}) that under fairly mild conditions, the intersection \( \mathcal{A} \cap \mathcal{B} \) is a singleton, up to a global phase ambiguity:
\begin{equation} \label{eq:phase_ambiguity}
    y_0 \in \mathcal{A} \cap \mathcal{B} \implies e^{i\theta}y_0 \in \mathcal{A} \cap \mathcal{B}, \quad \text{for any global phase } \theta.
\end{equation}

In the crystallographic phase retrieval problem, we assume the signal is sparse, imposing a sparsity constraint (see \cite{elser2018benchmark}) alongside the previously mentioned projection onto \( \mathcal{B} \).
Sparsity refers to the property where most elements of a vector are zero, with only a few non-zero entries. A vector is defined as \( S \)-sparse if it contains no more than \( |S| \) non-zero entries. The second projection enforces sparsity by retaining the \( |S| \) largest-magnitude elements of the vector and setting the rest to zero.


The projection operator \( P_S \) onto the space of \( S \)-sparse vectors is defined as follows: for a given vector \( x \in \mathbb{C}^n \), \( P_S(x) \) is obtained by:

\[
P_S(x)[i] = 
\begin{cases}
x[i], & \text{if } i \text{ corresponds to one of the } |S| \text{ largest elements of } |x|, \\
0, & \text{otherwise}.
\end{cases}
\]
This projection ensures that the sparsity constraint is enforced on the signal, keeping the \( |S| \) largest-valued pixels unchanged while setting the rest to zero. 
Note that we impose the sparsity constraint on our vector \( x \) rather than on vector \( y \) as before. However, it is important to highlight that in the crystallographic phase retrieval problem, the matrix \( A \) is invertible (IDFT). Therefore, it does not matter whether we apply the mappings to \( y \) or \( x \), as long as we remain consistent in our approach. 

\section{Projections for the matrix completion problem}

In the case of Matrix Completion, the projections are different from the phase retrieval cases. The first projection enforces the known entries of the matrix, ensuring that the matrix matches the given values at specific locations. This can be represented using a mask operator that preserves the known entries and leaves the unknown entries unchanged. The second projection imposes a rank constraint, forcing the matrix to have a rank of \( r \), which is assumed based on additional information.

Let \( M \in \mathbb{R}^{m \times n} \) represent the matrix we want to complete, and let \( \Omega \) denote the set of indices where the matrix entries are known. We define the mask operator \( P_{\Omega} \) as follows:

\[
P_{\Omega}(X,M)[i,j] = 
\begin{cases} 
M[i,j], & (i,j) \in \Omega, \\
X[i,j], & (i,j) \notin \Omega,
\end{cases}
\]
where \( X \) is the current estimate of the matrix. This projection ensures that the given matrix elements remain unchanged, while the unknown elements can still be updated in subsequent iterations.

The second projection forces the matrix to have a rank of \( r \). This is done using the Singular Value Decomposition (SVD). Let us denote the SVD of matrix \( M \) by:

\[
M = U \Sigma V^*
\]
where \( \Sigma \) is the diagonal matrix of singular values, and \( U \), \( V \) are orthogonal matrices. The projection onto the set of rank-\( r \) matrices, denoted as \( P_r(M) \), is given by:

\[
P_r(M) = U \Sigma_r V^*
\]
where \( \Sigma_r \) is obtained by retaining only the largest \( r \) singular values and setting the rest to zero. This projection ensures that the resulting matrix has rank \(r\), this is the best rank-\(r\) approximation of the matrix.




\section{Set Projection Example Illustrated with Sudoku}
\label{sec:set_projection_sudoku}

One effective method for solving Sudoku is the projections method, which I bring up because it helps visualize the problem intuitively (see Figure~\ref{fig:sudokuConstraints}). Let us consider the problem space as the set of all possible arrangements of the numbers 1-9 within the cells of a Sudoku matrix. To solve the puzzle, we must adhere to several rules.

\begin{enumerate}
    \item \textbf{Initial Clues:} We must respect the initial numbers provided at the start of the game.
    \item \textbf{Sudoku Rules:} Each column, row, and $3\times3$ block must contain all the numbers 1-9 exactly once.
\end{enumerate}
Each of these rules acts as a \textit{constraint} on our solution. These constraints define subspaces within the larger problem space. Solving the Sudoku puzzle means finding the intersection of all these subspaces, ensuring that every constraint is satisfied.

At this stage, it is useful to define the problem using spaces and subspaces, as described in~\eqref{eq:model2}. After establishing this framework, the next goal is to find the intersection point using a computationally efficient method. The process begins by selecting a random initial vector within the large problem space. Then, for each constraint, we define a corresponding subspace and project the vector onto it. This projection adjusts the vector to satisfy that specific constraint.
However, challenges arise when multiple constraints are involved. If the solution satisfies one constraint and is then projected onto the subspace of a second constraint, it may no longer satisfy the first. A \textit{naive approach} to solving Sudoku involves iteratively projecting onto each constraint in sequence. For example, we project onto the first constraint, then the second, and so on, cycling through all constraints repeatedly in the hope that the solution will converge.
Later, we will explore more advanced methods of projection to improve this process and achieve a smarter, more efficient solution.


\begin{figure}[ht]
  \centering
  \resizebox{0.8\textwidth}{!}{ % Scale down the entire figure
    \begin{minipage}{\textwidth}
      \centering
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_1}
        \caption{Row constraint: Each row must contain the numbers 1-9.}
        \label{fig:sudokuRow}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_2}
        \caption{Column constraint: Each column must contain the numbers 1-9.}
        \label{fig:sudokuColumn}
      \end{subfigure}
  
      \vspace{0.5cm}
  
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_3}
        \caption{Block constraint: Each of the 9 distinct 3x3 block must contain the numbers 1-9.}
        \label{fig:sudokuBlock}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_4}
        \caption{Solution: The completed Sudoku grid where all constraints are satisfied.}
        \label{fig:sudokuSolution}
      \end{subfigure}
    \end{minipage}
  }
  \caption[Examples of Sudoku constraints]{Examples of Sudoku constraints include rows, columns, blocks, and the solution itself. Each constraint requires that the numbers 1-9 appear exactly once in the corresponding row, column, or block. The solution is achieved when all constraints are satisfied.}
  \label{fig:sudokuConstraints}
\end{figure}

\chapter{Projection-based algorithms}
\label{chap:algorithm_iterative_steps}

In this chapter, we present the mathematical formulation of the algorithms used to solve the problems. Additionally, we will outline their respective stopping criteria. Each algorithm exhibits different behaviors, which will become clearer as we progress. As a general rule, all the algorithms will begin from a random starting point in the space. The algorithms we will focus on are:


\begin{itemize}
    \item \textbf{Alternating Projections (AP)}
    \item \textbf{Hybrid Input-Output (HIO)}
    \item \textbf{Relaxed Averaged Alternating Reflections (RAAR)}
    \item \textbf{Relaxed Reflect Reflect (RRR)}
\end{itemize}
With projection already thoroughly explained, and before diving into the specifics of each algorithm, it is important to provide a more detailed explanation of reflection and relaxation, as these principles form the foundation of the iterative steps employed in the methods to be discussed.

\subsection*{Reflection}

Reflection of a vector $\vec{v}$ across another vector $\vec{l}$(line) is a geometric operation that creates a new vector $\operatorname{Ref}_l(\vec{v})$, which is the symmetric counterpart of $\vec{v}$ with respect to $\vec{l}$. The reflection is defined as:
\[
\operatorname{Ref}_l(\vec{v}) = 2 \operatorname{Proj}_l(\vec{v}) - \vec{v},
\]
where $\operatorname{Proj}_l(\vec{v})$ is the projection of $\vec{v}$ onto $\vec{l}$. While we have discussed in detail how the projection is computed, for the purpose of the following example, we will assume it is precomputed as:
\[
\operatorname{Proj}_l(\vec{v}) = \frac{\vec{v} \cdot \vec{l}}{\vec{l} \cdot \vec{l}} \vec{l}.
\]
Alternatively, the formula can be expressed as:
\[
\operatorname{Ref}_l(\vec{v}) = \vec{v} + 2 (\operatorname{Proj}_l(\vec{v}) - \vec{v}),
\]
which can be interpreted geometrically: the projection $\operatorname{Proj}_l(\vec{v})$ represents the closest point on $\vec{l}$ to $\vec{v}$, and doubling the distance $(\operatorname{Proj}_l(\vec{v}) - \vec{v})$ reflects $\vec{v}$ symmetrically onto the other side.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{figures1/reflection_plot.png}
    \caption[Reflection of a vector across a line.]{Reflection of $\vec{v} = (3, 4)$ across $\vec{l} = (1, 1)$. The reflection demonstrates the symmetry of the operation.}
    \label{fig:reflection}
\end{figure}


To illustrate, consider the vector $\vec{v} = (3, 4)$ and the vector $\vec{l} = (1, 1)$. We compute step by step:

1. Compute the projection:
   \[
   \operatorname{Proj}_l(\vec{v}) = \frac{(3)(1) + (4)(1)}{(1)^2 + (1)^2} (1, 1) = \frac{7}{2}(1, 1) = (3.5, 3.5).
   \]

2. Compute twice the projection:
   \[
   2 \operatorname{Proj}_l(\vec{v}) = 2 \cdot (3.5, 3.5) = (7, 7).
   \]

3. Compute the reflected vector:
   \[
   \operatorname{Ref}_l(\vec{v}) = 2 \operatorname{Proj}_l(\vec{v}) - \vec{v} = (7, 7) - (3, 4) = (4, 3).
   \]
In the accompanying figure (Figure~\ref{fig:reflection}), you can see:
The original vector $\vec{v}$ in blue, with coordinates $(3, 4)$.
Its projection $\operatorname{Proj}_l(\vec{v})$ in orange, with coordinates $(3.5, 3.5)$.
The reflected vector $\operatorname{Ref}_l(\vec{v})$ in green, with coordinates $(4, 3)$.
The line of reflection is in red.







\subsection*{Relaxation}
Relaxation introduces a degree of flexibility to projection or reflection operations, allowing the algorithm to adjust the strength of these operations. For instance, in a relaxed projection, the operation may take the form
\[
x_{k+1} = x_k + \beta \big(P_S(x_k) - x_k\big),
\]
where \( \beta \) is a relaxation parameter that controls the step size or influence of the projection. Similarly, relaxation can be applied to reflections, offering a balance between aggressive and conservative updates to the solution. This flexibility is critical for managing convergence behavior, especially in challenging optimization landscapes.
By integrating these fundamental operations with iterative strategies, the algorithms discussed in this chapter achieve both robust and efficient convergence.

\section{Alternating Projections (AP)}

The Alternating Projections method involves iteratively projecting onto different sets. The update step for this method is given by:
\begin{equation}
y^{(k+1)} = P_{\mathcal{A}} \left( P_{\mathcal{B}} \left( y^{(k)} \right) \right),
\end{equation}
where \( y^{(k)} \) is the vector at the \( k \)-th iteration, \( P_{\mathcal{A}}(y) \) and  \( P_{\mathcal{B}}(y) \) are the projection operators onto sets \(\mathcal{A} \) and \( \mathcal{B} \), respectively. This method alternates between projecting onto each set to iteratively approach a solution that satisfies all constraints. The main issue is that the process can sometimes become stuck at a specific value, causing all subsequent iterations to repeatedly produce the same result, as illustrated in Figure \ref{fig:algorithm_behaviors}.

\begin{figure}[h!]
    \centering

    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_1}
        \caption{\centering Start from a random point}
        \label{fig:proj_circle_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_2}
        \caption{\centering Projection on the first circle}
        \label{fig:proj_circle_b}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_3}
        \caption{\centering Projection on the second circle}
        \label{fig:proj_circle_c}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_4}
        \caption{\centering Projection on the third circle}
        \label{fig:proj_circle_d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_5}
        \caption{\centering First step of the algorithm}
        \label{fig:proj_circle_e}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_6}
        \caption{\centering Convergence to the intersection point}
        \label{fig:proj_circle_f}
    \end{subfigure}

    \caption[Illustration of AP algorithm using orthogonal projections.]{Illustration of AP algorithm. The figure shows iterations in a projection-based algorithm within a 2-dimensional space. The goal is to find the intersection point of three circles using orthogonal projections. The process starts from a random point and proceeds iteratively. These images are referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.}
    \label{fig:proj_circle}
\end{figure}




\noindent In Figure \ref{fig:proj_circle}, an illustration of iterations in a projection-based algorithm within a 2-dimensional space is provided. In this example, the goal was to find the intersection of three circles using orthogonal projections onto them. The process began from a random point and continued iteratively until convergence to the solution. It is important to note that there are cases where convergence may be slow or may fail. This image is sourced from Andrew Maiden's 2022 lecture \cite{Maiden2022}.


\section{Hybrid Input-Output (HIO)}

The Hybrid Input-Output (HIO) algorithm (\cite{fienup1982phase}) updates its iterative step as follows:
\begin{equation}
y^{(k+1)} = y^{(k)} + P_{\mathcal{A}} \left( (1 + \alpha) P_{\mathcal{B}}(y^{(k)}) - y^{(k)} \right) - \alpha P_{\mathcal{B}} \left( y^{(k)} \right),
\end{equation}
where \( \alpha \) is a parameter controlling the amount of feedback applied in the iteration process.

\section{Relaxed Averaged Alternating Reflections (RAAR)}

The Relaxed Averaged Alternating Reflections (RAAR) algorithm (\cite{luke2004relaxed}) updates its iterative step as:
\begin{equation}
y^{(k+1)} = \beta \left( y^{(k)} + P_{\mathcal{A}} \left( 2 P_{\mathcal{B}}(y^{(k)}) - y^{(k)} \right) \right) + (1 - 2 \beta) P_{\mathcal{B}} \left( y^{(k)} \right),
\end{equation}
where \( \beta \) is a parameter controlling the balance between the two projections \(P_{\mathcal{A}}\) and  \( P_{\mathcal{B}}\).

\section{Relaxed Reflect Reflect (RRR)}

The  Relaxed Reflect Reflect (RRR) algorithm (\cite{elser2017matrix}) updates its iterative step as:
\begin{equation}
y^{(k+1)} = y^{(k)} + \gamma \left( P_{\mathcal{A}} \left( 2 P_{\mathcal{B}}(y^{(k)}) - y^{(k)} \right) - P_{\mathcal{B}} \left( y^{(k)} \right) \right),
\end{equation}

where \( \gamma \) is a parameter similar to \( \beta \) in RAAR, determining the weight given to each projection. One advantage of this algorithm is that it oscillates, which helps it avoid getting stuck in a local minimum, unlike the AP method, see Figure~\ref{fig:algorithm_behaviors}. This oscillatory behavior often leads to excellent performance, as will be further discussed in the next chapter. Furthermore, similar to the HIO algorithm, it can be rigorously shown that under certain scenarios (see \cite{levin2019note}), the algorithm will never get stuck indefinitely. Instead, it will converge either to a valid solution or to a state where stagnation must necessarily imply the correct solution has been found. We will consider cases where \( \alpha \), \( \beta \), and \( \gamma \) lie in the interval \((0, 1)\). If all parameters are equal to one, all three algorithms will behave identically.


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/AP_example_stuck_1}
        \caption*{(a) AP algorithm getting stuck.}
        \label{fig:ap_stuck}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/RRR_example_oscillate_1}
        \caption*{(b) RRR exhibiting oscillations.}
        \label{fig:rrr_oscillate}
    \end{minipage}
    \caption[RRR and AP algorithms exhibit different behaviors.]{Illustrations of different algorithm behaviors: (a) AP algorithm getting stuck and (b) RRR algorithm oscillating. This is an example of running a composite random matrix \(A\) drawn i.i.d. from a normal distribution of size \(50 \times 15\). The run was stopped after 1000 iterations when the AP algorithm still did not converge at this stage, whereas the RRR algorithm had already converged. Convergence is defined by the stopping criteria outlined in Section~\ref{sec:stopping_criteria}.
}
    \label{fig:algorithm_behaviors}
\end{figure}


\section{Stopping Criteria} \label{sec:stopping_criteria}
A point \( y_0 \in \mathbb{C}^m \) is considered a solution if it satisfies the condition \( P_{\mathcal{A}}(y_0) = P_{\mathcal{B}}(y_0) \).  
Thus, the stopping criterion for most of our algorithms is to check whether the norm of the difference between the two projections is negligible. In practice, however, equality is often accepted within a certain tolerance. For most of our cases, the stopping criterion is expressed as:

\begin{equation}
\| P_{\mathcal{B}}(y_0) - P_{\mathcal{A}}(y_0) \| \leq \varepsilon,
\label{eq:stopping_criterion}
\end{equation}
where \( P_{\mathcal{A}} \) and \( P_{\mathcal{B}} \) are the projections onto sets \( \mathcal{A} \) and \( \mathcal{B} \), respectively, \( y_0 \in \mathbb{C}^m \) is the candidate solution, and \( \varepsilon \) is the tolerance value.  

For the crystallographic phase retrieval problem, as discussed extensively in \cite{elser2018benchmark}, we adopt a slightly different stopping criterion. Here, the stopping condition is defined as follows:
\begin{equation}
\mu \coloneqq \frac{\text{I}_\text{S}}{\text{I}_\text{F}}, \quad \mu > 0.95,
\label{eq:threshold_condition}
\end{equation}

where \(\text{I}_\text{F}\), the power of the entire image (sum of the squared values of all pixels), is given by:
\[
\text{I}_\text{F} = \sum_{u=0}^{M-1} \sum_{v=0}^{M-1} x^2(u, v),
\]
and \(\text{I}_\text{S}\), the power of the pixels within the support \( S \), is defined as:
\[
\text{I}_\text{S} = \sum_{(u,v) \in S} x^2(u, v).
\]
Here, \( x(u, v) \) represents the reconstructed pixel intensity at the coordinates \( (u, v) \) in the image. The support region \( S \) represents the expected sparsity of the image, indicating areas where the content is anticipated to be nonzero. It is worth noting that increasing the threshold above 0.95 will result in longer convergence times but can improve the reconstruction’s accuracy, essentially ensuring a lower tolerance.  


\chapter{Theoretical Boundaries in Matrix Completion}

\section*{Motivation: The Netflix Prize Model}

The Netflix Prize Model, as outlined in Figure~\ref{table:Netflix_table}, organizes users as rows and movies as columns. The purpose of this model is to recommend movies to users based on their preferences, inferred from ratings they provide for certain movies. These ratings can be represented as percentage values. A central question in this context is how many ratings need to be provided and how many can remain missing while still enabling accurate predictions. This challenge is closely tied to the matrix rank \(r\), which represents the inherent structure of the dataset. The matrix rank offers a mathematical representation of the patterns in user preferences and movie characteristics. For example, similar users might show comparable rating tendencies, or individuals could exhibit preferences for specific genres. However, alternative mathematical formulations, such as explicit connections between genres or clusters of similar users, could also describe these relationships. While such approaches are interesting and potentially relevant, they fall beyond the scope of this discussion. Here, we focus solely on understanding and utilizing the rank of the matrix. Our approach is motivated by the need to balance efficiency and effectiveness. Minimizing the effort required from users, such as avoiding unnecessary input requests, is crucial to creating a smooth user experience. At the same time, the system must maintain its ability to generate accurate recommendations. This balance underscores the fundamental trade-offs in matrix completion between user interaction and algorithmic accuracy. For the purposes of this study, we concentrate exclusively on cases where the matrix rank is assumed to be known, and we explore how this assumption influences the theoretical boundaries of the problem.




\section*{The Maximum Number of Elements that Can Be Deleted While Retaining Accurate Reconstruction}
The following result establishes the maximum number of elements that can be deleted from a matrix while still ensuring its unique reconstruction under the assumption that the rank of the matrix is known.

\subsection*{Claim}
Let \( A \) be a square \( n \times n \) matrix of rank \( r \). It is possible to delete up to:
\begin{equation}
\label{eq:deletable_elements}
q = n^2 - (2nr - r^2) = (n-r)^2,
\end{equation}
elements while still ensuring unique reconstruction of the matrix, assuming only the rank of the matrix and the known elements are given.

\subsection*{Proof}

\begin{enumerate}
    \item \textbf{Rank \( r \):} A rank-\( r \) matrix can be expressed as the product of two smaller matrices: \( A = UV^\top \), where:
    \begin{itemize}
        \item \( U \) is an \( n \times r \) matrix, and
        \item \( V \) is an \( r \times n \) matrix.
    \end{itemize}
    The total number of parameters required to describe \( A \) is:
    \[
    \text{Total parameters} = nr + nr = 2nr.
    \]

    \item \textbf{Non-uniqueness of the decomposition:} The decomposition \( A = UV^\top \) is not unique because \( U \) can be multiplied by any invertible \( r \times r \) matrix, and \( V \) can be multiplied by its inverse. This introduces a redundancy of \( r^2 \) parameters. Thus, the number of \textit{unique} parameters needed to describe \( A \) is:
    \[
    \text{Unique parameters} = 2nr - r^2.
    \]

    \item \textbf{Minimum number of known elements:} To uniquely reconstruct the matrix, at least \( 2nr - r^2 \) entries of the matrix must be known. 

    \item \textbf{Number of deletable elements:} Since the total number of elements in the matrix is \( n^2 \), the number of elements that can be deleted while still allowing for unique reconstruction is:
    \[
    q = n^2 - (2nr - r^2)  = (n-r)^2.
    \]
\end{enumerate}

\section*{Example: Matrix Reconstruction in the Best-Case Scenario}

We consider the following matrix \( A \) of size \( n = 5 \) with rank \( r = 3 \):

\[
A =
\begin{bmatrix}
1 & 1 & 1 & 6 & 3 \\
1 & 2 & 3 & 12 & 6 \\
1 & 3 & 4 & 16 & 8 \\
9 & 18 & 24 & ? & x \\
3 & 6 & 8 & ? & ? \\
\end{bmatrix}.
\]
This is a best-case scenario where: The first \( r = 3 \) rows and the first \( r = 3 \) columns are linearly independent.
Here, we have removed \((n-r)^2 = (5-3)^2 = 4\) entries from the matrix. 
If we wish to reconstruct the element \(x\), it is evident that the fifth column is a linear combination of the first three columns by definition. This can be directly observed by summing one entry from each of the first three columns. Once this relationship is established, to determine \(x\), we simply sum the entries from the first three columns in the fourth row, yielding \(x\). Similarly, this approach can be applied to reconstruct other unknown entries as well. It is important to note that if additional elements were removed, we might still succeed in solving for \(x\), though the solution might not be unique. Conversely, if fewer elements were removed, fully reconstructing the matrix might become impossible. However, in certain cases, reconstruction could still be feasible if the matrix is observed or assumed to follow a specific known pattern.




\subsection*{Limitation of This Discussion}

This result assumes only two constraints:
\begin{enumerate}
    \item The rank \( r \) of the matrix is known.
    \item A sufficient number of matrix elements are known (i.e., \( n^2 - q \) elements remain).
\end{enumerate}
We do not consider cases where additional information or a third constraint is provided (e.g., sparsity, structural patterns, or specific relationships among matrix elements).

\subsection*{Testing Algorithm Stability Within Theoretical Bounds}
Based on this proof, the simulation in the Numerical Experiment section will examine the performance of the algorithm as a function of the number of missing elements, without exceeding the theoretical bound established here.

\chapter{Numerical Experiments}
In this chapter, the numerical results are presented, accompanied by figures and simulations to clarify and illustrate the findings. Various cases are examined to provide context and depth to the analysis, comparing different algorithms to showcase their performance in each scenario. The data is explained and interpreted, offering insights and a detailed discussion supported by the simulations included in the attached code.
% \newpage
\section{Random Phase Retrieval Problem}

In this section, we describe the steps of our experiment and present the results and conclusions for the random phase retrieval problem. Specifically, we generated a random  i.i.d. matrix \( A \in \mathbb{C}^{m \times n} \) and a random vector \( x \in \mathbb{C}^n \), where the entries of \( A \) and \( x \) are independently drawn from a standard Gaussian normal distribution \( \mathcal{N}(0, 1) \) for the real and imaginary parts. As mentioned earlier, the iterations are performed on the vector $y$, which is computed as the product of $A$ and $x$, i.e., \( y = Ax \in \mathbb{C}^m \). In our system, we incorporate the magnitude of the product $y$ along with the matrix $A$, which is also provided. Note that our vectors and matrices are defined in the complex domain. 
The dimensions and parameters used in the experiment are:
\begin{itemize}
    \item $m = 25$, $n = 10$
    \item $\beta = 0.5$, which was observed to be optimal in the vast majority of cases.
\end{itemize}
The vector \(y\) is initialized randomly, and we implement the following algorithms to solve the problem:

\begin{itemize}
    \item \textbf{AP}
    \item \textbf{RRR}
    \item \textbf{RAAR}
    \item \textbf{HIO}
\end{itemize}
We examine and analyze the performance of each algorithm under these conditions.
Our method demonstrates convergence when two iterations produce nearly identical outcomes. In this case, the convergence is achieved within an error margin of \(10^{-6}\).
We will present the convergence plots to visualize this behavior. Additionally, I will include a graphical representation that I find helpful for visualizing the results: showing the values of a vector as a function of its index.
\begin{figure}[h!]
    \centering
    \hspace*{-1cm}
    \includegraphics[width=1.1\textwidth]{phase_complex/vector_representation.png}
    \caption{Graphical representation of a vector as a function of its index.}
    \label{fig:vector_representation}
\end{figure}


After understanding the methodology, we now present the results of our experiment, where we ran four algorithms and display the vector values as previously explained. We defined convergence as the achievement of the desired criteria \eqref{eq:stopping_criterion} within a tolerance of \( 10^{-6} \) and a maximum of 100,000 iterations. Specifically, the norm of the difference between the two projections \( P_{\mathcal{A}} \) and \( P_{\mathcal{B}} \) must satisfy:

\[
\| P_{\mathcal{A}}(y_0) - P_{\mathcal{B}}(y_0) \| \leq 10^{-6}.
\]
The iteration limit, or maximum number of iterations, refers to the upper bound on the number of iterations the algorithm is allowed to perform. If the stopping criterion \eqref{eq:stopping_criterion} is not satisfied within this limit, the algorithm is considered to have failed to converge. This ensures that the algorithm terminates even when convergence is not achieved, thereby preventing infinite or excessively long computations.

The statement in Eq.~\eqref{eq:phase_ambiguity} indicates that the solution is determined uniquely up to a global phase ambiguity. Therefore, there is no point in presenting our solution alongside the ground truth, as visually, the values do not necessarily need to match and could involve a shift. To better understand how our solution approaches the ground truth vector \( y_0 \) (where \( x_0 \) is the original ground truth, and \( y_0 = A x_0 \)), we use a graphical representation as shown in Fig.~\ref{fig:results_m_25__n_10}. After obtaining our estimated solution \( \hat{y} \), we project it onto \( \mathcal{A} \) to ensure it satisfies the constraint associated with \( \mathcal{A} \).
Next, we check whether \( \hat{y} \) satisfies the constraint associated with \( \mathcal{B} \), which is defined by the projection operator \( P_{\mathcal{B}} \). Since the set \( \mathcal{B} \) is defined by a magnitude constraint, we compute the magnitude of \( \hat{y} \) and compare it to \( b \). Specifically, we verify the condition:

\begin{equation}
    |\hat{y}| = b,
\end{equation}
where \( b \) represents the magnitude constraint defined by \( \mathcal{B} \). If the magnitude of \( \hat{y} \) aligns with \( b \), it confirms that \( \hat{y} \) satisfies the constraint for \( \mathcal{B} \). Combined with the earlier projection onto \( \mathcal{A} \), this ensures that \( \hat{y} \) meets both constraints and corresponds to a valid solution of the problem.

We present the results of our experiment, in which we ran four algorithms, with only two successfully converging. The RRR algorithm converged after 493 iterations, and the HIO algorithm converged after 726 iterations. As shown in Fig.~\ref{fig:results_m_25__n_10}, the HIO and RRR algorithms achieved convergence, satisfying the desired criteria \eqref{eq:stopping_criterion} within 100,000 iterations and aligning closely with the original \( b \) vector. In contrast, the other algorithms did not meet the stopping criterion, although their results were not very far from it.




\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{phase_complex/m_25__n_10__beta_0_5__e_6_max_iter_100000_RRR_493_HIO_726.png}
    \caption[Final results of the algorithms.]{
    The magnitude of the estimated solution \( \hat{y} \) after projection onto \( \mathcal{A} \) (using \( P_{\mathcal{A}} \)) is compared to \( b \), which represents the magnitude of the ground truth vector \( y_0 \). The figure demonstrates how the HIO and RRR algorithms fully satisfy the solution, as they meet both constraints: first by ensuring \( \hat{y} \) satisfies the \( \mathcal{A} \)-constraint through projection and then verifying that it also satisfies the magnitude constraint \( b \). In contrast, the other algorithms did not meet the stopping criterion, although their results were not very far from it. The parameters used in this experiment were \( m = 25 \), \( n = 8 \), and \( \beta = 0.5 \).}


    \label{fig:results_m_25__n_10}
\end{figure}

The convergence behavior of all four algorithms can be observed, with the solution defined by the projections onto the two sets being identical or nearly identical. To illustrate this, we present a figure showing the norm value between the projections as a function of the iteration count. See Figure~\ref{fig:4_convergence_fig} for a visual representation of the convergence behavior of the algorithms. As can be seen, both AP and RAAR fail to reach our tolerance level, and their values remain high.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/AP_C.png}
        \caption{AP}
        \label{fig:convergence_ap}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/RRR_C.png}
        \caption{RRR}
        \label{fig:convergence_rrr}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/HIO_C.png}
        \caption{HIO}
        \label{fig:convergence_hio}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/RAAR_C.png}
        \caption{RAAR}
        \label{fig:convergence_raar}
    \end{subfigure}
    \caption[The norm between the projections as a function of the number of iterations.]{The difference between the projections as a function of the number of iterations. It can be observed that only the RRR and HIO algorithms converged, where convergence is defined as achieving a difference within the tolerance between the projections(satisfying the stopping criterion \eqref{eq:stopping_criterion}), within the limit of the maximum number of iterations (100,000). The parameters used here are the same as before: \( m = 25 \), \( n = 8 \), and \( \beta = 0.5 \).}

    \label{fig:4_convergence_fig}
\end{figure}

\subsection{Comparison of Algorithms for the Random Phase Retrieval Problem}


Now we present a statistical analysis averaging over 10,000 trials. The objective of this experiment is to evaluate the performance of four algorithms under specific conditions where $m = 25$, $n = 8$, and $\beta = 0.5$. Each algorithm was tested on an identical dataset, and the aggregated results are shown in the figure below.
Figure ~\ref{fig:percentage_results} illustrates the performance distribution of the four algorithms across the 10,000 trials. This includes the average success rates and the variability in performance.
The results clearly show that the AP and RAAR algorithms exhibit similar performance levels, achieving moderate success rates. On the other hand, RRR and HIO show significantly better performance, with both achieving substantially higher success rates compared to the other two methods. This indicates that these algorithms are more robust and adaptable, likely benefiting from their unique iterative approaches. Among RRR and HIO, their performance differences are minor, but further analysis could reveal specific scenarios where one has a distinct advantage over the other. Overall, these results emphasize the strengths of each algorithm and provide a foundation for selecting the most suitable method depending on the problem context.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{phase_complex/percentage.png} 
    \caption[Performance distribution of 10,000 trials for the four algorithms.]{
    Performance distribution of 10,000 trials for the four algorithms, representing the success rates (convergence percentages, satisfying the stopping criterion).  
    The results demonstrate that two of the algorithms (RRR and HIO) achieved significantly higher success rates, indicating their superior performance in meeting the convergence criteria. The parameters used were \( m = 25 \), \( n = 8 \), the maximum number of iterations was 1000, the tolerance was \( 10^{-4} \), and \( \beta = 0.5 \).
    }
    \label{fig:percentage_results}
\end{figure}


Beyond understanding the convergence rates, we are also interested in analyzing the number of iterations required for each algorithm to converge. To illustrate this, we include two images in Figure~\ref{fig:convergence_iterations_fig}. The first panel shows the number of iterations required for each trial to converge for each algorithm, while non-convergent trials are assigned the maximum number of iterations. The second panel offers a zoomed-in view, allowing for clearer observation of subtle differences in convergence behavior across trials.
The results demonstrate a clear overlap between the convergence percentages and the number of iterations required for convergence. Specifically, the HIO algorithm converges slightly more often than the RRR algorithm, and both significantly outperform the other two algorithms. It is important to note that while there are instances where the RRR algorithm required more iterations to converge compared to the AP or RAAR algorithms, there is no meaningful comparison between them. The RRR algorithm delivered outstanding performance overall, significantly exceeding the effectiveness of AP and RAAR.
\begin{figure}[h!]
    \centering
    % Subfigure (a)
    \begin{subfigure}[t]{\textwidth} % Use full text width for alignment
        \centering
        \includegraphics[width=0.8\textwidth]{phase_complex/Converged_values.png}
        \caption{ \centering Iterations required for convergence for each algorithm.}
        \label{fig:converged_value}
    \end{subfigure}
    \vspace{1em} % Add small vertical space
    % Subfigure (b)
    \begin{subfigure}[t]{\textwidth} % Use full text width for alignment
        \centering
        \includegraphics[width=0.8\textwidth]{phase_complex/zoom_in.png}
        \caption{ \centering Zoomed-in view of convergence behavior for better visualization.}
        \label{fig:zoom_in}
    \end{subfigure}
    
    % Main caption
    \caption[Analysis of iterations required for convergence across all trials for each algorithm.]{(a) Iterations required for convergence for each algorithm. (b) Zoomed-in view of convergence behavior for better visualization. Appears in the figure the number of iterations to convergence (i.e., to satisfy the criterion), or the maximum value if the criterion was not met within the maximum number of iterations.}
    \label{fig:convergence_iterations_fig}
\end{figure}




\section{Crystallographic Phase Retrieval Problem}
In this scenario, we are addressing a realistic case where the vector \( x \) is sparse. We apply a Fourier transform to \( x \), as discussed previously. The experimental setup is similar to earlier experiments, but here we specifically highlight the sparsity of \( x \), meaning it contains only \( S \) non-zero entries. As mentioned before, the stopping condition for our process is based on a ratio \( \mu \), defined in Equation~\eqref{eq:threshold_condition}. The results of the experiment, conducted with a threshold of 0.95, are shown in Figure~\ref{fig:experiment_0_95}. The parameters used for the experiment include \( n = 40 \) (a \( 40 \times 40 \) Discrete Fourier Transform (DFT) matrix) and \( S = 4 \). To demonstrate the method, we applied the RRR algorithm. Figure~\ref{fig:abs_fft_0_95} illustrates the reconstructed vector values. The y-axis represents the magnitudes of the vector entries, as explained in Figure~\ref{fig:vector_representation}, where we explicitly show the values of the vector as a function of their indices. The plot compares the magnitudes of our reconstructed vector (after projecting with \( P_{\mathcal{A}} \)) with the original vector \( b \), as described in the introduction to Figure~\ref{fig:results_m_25__n_10}. In this case, the magnitude of \( y \), since \( y = A x \) and \( A \) is the DFT matrix, is simply the magnitude of the Fourier transform \( | \text{DFT}(x) | \). This is what we present in the graph alongside \( b \).
Finally, Figure~\ref{fig:threshold_ratio} shows the ratio \( \mu \), as defined in Equation~\eqref{eq:threshold_condition}, plotted against the number of iterations. This plot illustrates the success curve, showing how the ratio converges toward the target value of 0.95.  


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dft_case/original/abs_fft_0_95}
        \caption{Reconstructed vector values: This figure shows the magnitude of the vector after applying the DFT operation, and compared with the original vector \(b\). The y-axis represents the magnitude of the values of the reconstructed vector as a function of the index.}
        \label{fig:abs_fft_0_95}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \hspace{-1cm}
        \includegraphics[width=\textwidth]{dft_case/original/th_0_95}
        \caption{Ratio as a function of iterations: This figure shows how the ratio evolves over multiple iterations of the RRR algorithm, converging towards the desired value of 0.95. The x-axis represents the iteration number, and the y-axis shows the ratio  \(\mu\) defined by Equation~\eqref{eq:threshold_condition}.}
        \label{fig:threshold_ratio}
    \end{subfigure}
    \caption[Results of the experiment with a threshold of 0.95]{Results of the experiment with a threshold of 0.95, showing both the reconstructed vector values and the ratio evolution over iterations. The parameters for the experiment are a \(40 \times 40\) DFT matrix and \(S = 4\).}

    \label{fig:experiment_0_95}
\end{figure}

While the results for the threshold of 0.95 are satisfactory, increasing the threshold to 0.999 leads to a better approximation of the original vector, as the reconstructed vector becomes much closer to the original. However, this improvement comes at the cost of requiring significantly more iterations for convergence (achieving the desired criteria). Figure~\ref{fig:experiment_0_999} depicts the same experiment as Figure~\ref{fig:experiment_0_95}, except that the threshold value is set to 0.999.
As shown in Figure~\ref{fig:experiment_0_999}, the reconstructed vector is significantly closer to the original \( b\) when the threshold is increased to 0.999, resulting in a better approximation. As expected, the ratio gradually converges towards 0.999 over more iterations, reflecting the additional effort required to achieve higher precision in the reconstruction. This highlights the trade-off between obtaining a more accurate approximation and the increased number of iterations needed for convergence.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dft_case/original/abs_fft_0_999}
        \caption{Reconstructed vector values for threshold 0.999: This figure shows the magnitude of the reconstructed vector values after applying the DFT operation. The reconstructed vector is much closer to the original vector \( b\) when the threshold is raised to 0.999. The y-axis represents the magnitude of the values of the reconstructed vector as a function of the index.}
        \label{fig:abs_fft_0_999}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \hspace{-1cm}
        \includegraphics[width=\textwidth]{dft_case/original/th_0_999}
        \caption{Ratio as a function of iterations for threshold 0.999: This figure shows the ratio as it evolves over multiple iterations. The ratio converges towards the desired value of 0.999, but requires more iterations to do so, illustrating the trade-off between accuracy and the number of iterations required. The x-axis represents the iteration number, and the y-axis shows the ratio  \(\mu\) defined by Equation~\eqref{eq:threshold_condition}.}
        \label{fig:threshold_ratio_0_999}
    \end{subfigure}
    \caption[Results of the experiment with a threshold of 0.999]{Results of the experiment with a threshold of 0.999, showing both the reconstructed vector values and the ratio evolution over iterations. The higher threshold leads to a better approximation of the original vector, but at the cost of requiring more iterations for convergence. The parameters used are the same as before: a \(40 \times 40\) DFT matrix, \(S = 4\).}
    \label{fig:experiment_0_999}
\end{figure}





\subsection{Effect of Adding Noise to Observations}
We can investigate the performance of the algorithm under noisy conditions. In our experiment (Figure~\ref{fig:convergence_iterations}), we added Gaussian noise with a mean of zero and a variance $\sigma^2$ to the input vector $x$. Specifically, we sampled 300 values for $\sigma$ in equal intervals ranging from $0$ to $10$. The noisy vector can be mathematically expressed as:

\[
b_{\text{noisy}} = b + n, \quad n \sim \mathcal{N}(\mathbf{0}, \sigma^2 I),
\]
where $\mathcal{N}(0, \sigma^2 I)$ represents a multivariate Gaussian distribution with a mean vector $\mathbf{0}$ and a covariance matrix $ \sigma^2 I$ (an identity matrix scaled by $\sigma^2$).


The results show that as the noise level increases, it becomes increasingly difficult for the algorithm to converge. In other words, it becomes harder for the algorithm to achieve the criterion \eqref{eq:threshold_condition}.
This is reflected in the number of iterations required and the density of points in the final stages. For high noise levels, the convergence nearly disappears, indicating that the algorithm struggles to settle as $\sigma$ approaches higher values. Points that did not converge are not visible on the figure.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{dft_case/original_noise/con_value_map}
    \caption[The number of iterations required for convergence as a function of $\sigma$ (noise variance).]{
    The number of iterations required for convergence as a function of $\sigma$ (noise variance). It is evident from the figure that higher noise levels lead to a significant increase in the iterations required for convergence.  
    Points where convergence was not achieved are not shown in the figure. Therefore, less dense regions indicate that only a few trials successfully converged. The parameters used were \( n = 40 \), \( S = 4 \), a maximum number of iterations of 10,000, and a convergence threshold of 0.95, according to the stopping criterion \eqref{eq:threshold_condition}.}
    \label{fig:convergence_iterations}
\end{figure}



From the data, the lowest $\sigma$ value for which the algorithm failed to converge was $2.27$. This is illustrated in Figure~\ref{fig:first_non_convergence}. Beyond this value, although some instances managed to converge, most failed to exhibit consistent convergence behavior. Additionally, the oscillatory nature of the algorithm can be observed in this figure, which explains why it performs well in many other cases despite the added noise. It is worth noting that while the algorithm failed to converge at $\sigma = 2.27$ in this experiment (Figure~\ref{fig:first_non_convergence}), there were iterations where it reached values for which the ratio was high. In such iterations, even though the lack of precision might be significant, the results could still be considered sufficient, depending on the requirements of the task.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{dft_case/original_noise/first_non_con}
    \caption[The first noise variance where the RRR algorithm failed to converge.]{The figure presents the ratio  \(\mu\) defined in \eqref{eq:threshold_condition} as a function of the number of iterations. This is the lowest noise variance (\(\sigma = 2.27\)) where the RRR algorithm failed to converge. The behavior of the RRR, as noted previously, is characterized by oscillations, as can be seen in the figure.}
    \label{fig:first_non_convergence}
\end{figure}
The results emphasize the impact of noise on the algorithm's convergence and highlight the challenges in achieving stability under noisy conditions.

\subsection{Comparison of Algorithms for the Crystallographic Phase Retrieval Problem}

After understanding how we conduct the simulation, we aim to examine not just a single case but a large number of experiments to gain a broad statistical perspective. We run 10,000 experiments with a matrix size of $n=50$ (i.e., $50 \times 50$ matrices) and $S=5$. We analyze the convergence percentages for each algorithm (see Figure ~\ref{fig:percentage_dft}). Note that 10,000 is the maximum number of iterations considered for convergence; if an algorithm does not converge within this limit, it is treated as non-convergent.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{dft_case/generic/percentage}
    \caption[Convergence percentages for different algorithms over 10,000 experiments.]{Convergence percentages for different algorithms over 10,000 experiments. It can be observed that two of the algorithms (RRR and HIO) demonstrated significantly higher convergence rates compared to the others, highlighting their robustness in achieving the convergence threshold. The parameters used were \( n = 50 \), \( S = 5 \), the maximum number of iterations was 10,000, \( \beta = 0.5 \), and a convergence threshold of 0.95, according to the stopping criterion \eqref{eq:threshold_condition}.}

    \label{fig:percentage_dft}
\end{figure}

Additionally, we also examine the convergence values, specifically how many iterations were required until convergence. Figure~\ref{fig:iterations} shows this analysis with two subplots. The first plot illustrates the iteration count until convergence as a function of the experiments. The second plot provides a zoomed-in view of the first plot, allowing a more detailed observation of the behavior. It is important to note that for experiments where the algorithm did not converge within 10,000 iterations, the corresponding data point is assigned the maximum number of iterations and is included in the plot.


\begin{figure}[h!]
    \centering
    % Subfigure (a)
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/Converged_values.png}
        \caption{Iterations required for convergence for each algorithm.}
        \label{fig:converged_value}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/zoom_in.png}
        \caption{Zoomed-in view of convergence behavior for better visualization.}
        \label{fig:zoom_in}
    \end{subfigure}

    % Main caption
    \caption[Analysis of iterations required for convergence across all trials for each algorithm.]{(a) Iterations required for convergence for each algorithm. (b) Zoomed-in view of convergence behavior for better visualization. Appears in the figure the number of iterations to convergence (i.e., to satisfy the criterion), or the maximum value if the criterion was not met within the maximum number of iterations.}
    \label{fig:convergence_iterations_fig}
\end{figure}
We turn our attention to analyzing the results illustrated in the figure. The data reveals significant differences in performance across the algorithms. RRR stands out as the most robust, demonstrating consistently high convergence rates compared to the other methods. Its reliability across various scenarios highlights its effectiveness and versatility.
In terms of iteration count, RRR further distinguishes itself by requiring fewer iterations to converge compared to HIO. However, in cases where all algorithms successfully converge, both AP and RAAR occasionally achieve convergence in fewer iterations than RRR. Despite these exceptions, the overall performance gap remains substantial. RRR not only excels in its ability to converge but also does so efficiently, solidifying its position as the superior algorithm for the problem at hand.

% \newpage
\section{Matrix Completion}
I will begin by illustrating how the basic simulation was performed, using a simple example to lay the groundwork for understanding the methodology. Once the foundation is established, we will delve into more advanced concepts, exploring their intricacies and implications in greater detail.
As we know, our problem involves reconstructing a correct matrix, which I will refer to as the ground-truth matrix, using the initial clues (the elements we know), along with the additional information about the matrix's rank. This framework serves as the basis for our approach, guiding the reconstruction process to ensure accuracy and alignment with the given constraints.

In the example shown in Figures \ref{fig:example_matrices}, you can see exactly how the simulation is performed. First, we created an $11 \times 11$ matrix with rank 3 and randomly selected 25 indices from the matrix to discard. We then provided two inputs to our code. The first input is the matrix we created after removing the 25 elements corresponding to the selected indices (this input is called the \textit{hint matrix}), and the second input is the information about the rank of the matrix, which in our case is 3.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/true.png}
        \caption{The ground-truth matrix (original matrix).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/given.png}
        \caption{The hint matrix, along with the additional information that the matrix has a rank of 3, was provided.}
    \end{subfigure}
    \caption{Illustration of the original and hint matrices used in the simulation.}
    \label{fig:example_matrices}
\end{figure}
\noindent After receiving the inputs, we will begin solving our problem using the RRR algorithm to reconstruct the complete matrix. The process starts from a random initialization, as shown in Figure~\ref{fig:init_matrix}. For convenience, an element in the matrix will be colored green if it is similar to the value in the original matrix and red if it is different.
In the following (Figure~\ref{fig:iterations}), we present the first iteration, the 33rd iteration, and the final iteration of the RRR algorithm. This visualization demonstrates the convergence process, where more elements of the matrix progressively approach the values of the original matrix, turning green. By the 33rd iteration, a significant number of elements are already close to the original values, highlighting the algorithm's progress. It is important to note that the figures reflect the state of the matrix after enforcing the hint matrix projection. As a result, the red elements appear only in the missing entries, where there is no information from the hint matrix.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{Matrix_Completion/Example_of_running_2_matrices/init.png}
    \caption{The ground-truth matrix(left) and initial random matrix (right) used as the starting point for the RRR algorithm.}
    \label{fig:init_matrix}
\end{figure}

\begin{figure}[p]
    \centering
    \begin{subfigure}[t]{0.72\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/1th_iter.png}
        \caption{Iteration 1.}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.72\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/33th_iter.png}
        \caption{Iteration 33.}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.72\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/end.png}
        \caption{Final iteration.}
    \end{subfigure}
    \caption[Visualization of the first, 33rd, and final iterations of the matrix in the RRR algorithm]{Visualization of the first, 33rd, and final iterations of the matrix in the RRR algorithm, showing the progression of convergence. The ground-truth matrix(left) and current estimate matrix (right). Green elements indicate agreement with the ground-truth matrix, while red elements appear only in the missing entries after enforcing the hint matrix projection, before displaying.}
    \label{fig:iterations}
\end{figure}


In addition to presenting the matrix convergence visually, we evaluate the performance of the RRR algorithm by analyzing two key metrics over iterations. These metrics are plotted to provide deeper insights into the algorithm's behavior throughout the iterative process. The first panel in Figure~\ref{fig:performance_plots} illustrates the convergence behavior of the algorithm by showing how the norm difference \( |P_{\mathcal{B}}(y) - P_{\mathcal{A}}(y)| \) evolves as a function of the iteration number. This measure is particularly significant as it serves as the stopping criterion according to equation~\eqref{eq:stopping_criterion}, with convergence or a solution defined when the projections are identical or, in our case, sufficiently similar within a tolerance of \( 10^{-6} \).
The second panel in the same figure illustrates the norm difference between the current estimate matrix and the ground-truth matrix, highlighting the accuracy of the reconstruction over iterations. It is important to note that this figure is not available during real-time execution, as it relies on access to the ground-truth matrix, which is only known for evaluation purposes.
Together, these plots provide a comprehensive view of the convergence and effectiveness of the algorithm. 
This example represents a relatively simple case, designed to demonstrate how the simulation operates. In the following sections, we will focus on more complex scenarios, where we are particularly interested in analyzing convergence or divergence, along with statistics and percentages derived from a larger set of experiments. These experiments will explore a broader range of parameter variations, such as matrix size, rank, and the proportion of missing entries.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \hspace*{-1cm}
        \includegraphics[height=0.75\textwidth]{Matrix_Completion/Example_of_running_2_matrices/convergence.png}
        \caption{Norm difference \( |P_{\mathcal{B}}(y) - P_{\mathcal{A}}(y)| \) as a function of the iteration number.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \hspace*{-1cm}
        \includegraphics[height=0.75\textwidth]{Matrix_Completion/Example_of_running_2_matrices/convergence2.png}
        \caption{Norm difference between the current estimate matrix and the ground-truth matrix over iterations.}
    \end{subfigure}
    \caption[Convergence and reconstruction errors of the RRR algorithm]{Convergence and reconstruction errors of the RRR algorithm. The first panel shows the error between projections at each iteration, while the second shows how closely the reconstructed matrix approximates the ground-truth matrix. Note that the second panel is generated only for evaluation purposes, as the ground-truth matrix is not available during real-time execution.}
    \label{fig:performance_plots}
\end{figure}




\subsection{Comparison of Algorithms for Matrix Completion}

Now we investigate the performance of three matrix completion algorithms: AP, RRR and RAAR. The goal is to evaluate their ability to converge under a specific experimental setup. In this scenario, the matrix has a size of $n = 20$, a rank of $r = 3$, and $q = 50$ randomly missing elements. The convergence criterion requires that the solution must be achieved in no more than 1000 iterations.

To ensure a statistically robust analysis, the experiment was repeated $10,000$ times independently. The success rate of each algorithm—defined as the percentage of trials where convergence occurred—is summarized in Figure~\ref{fig:success_percentage}. These results provide valuable insights into the relative strengths of the algorithms under the given constraints.
From the results, it is evident that all three algorithms demonstrate high levels of reliability, with success rates consistently exceeding $97\%$. However, subtle differences between the algorithms offer meaningful insights. The RRR algorithm emerges as the most robust, achieving the highest convergence rate across all trials. This suggests that RRR is particularly well-suited for matrix completion tasks where a high level of precision and consistency is required.
The RAAR algorithm, while slightly less consistent than RRR, still performs remarkably well, demonstrating that it is a strong alternative in scenarios where computational constraints or other practical considerations might favor its use. Finally, AP, despite its slightly lower success rate, provides a simpler approach that remains effective for most practical cases. This highlights a potential trade-off between algorithmic complexity and reliability.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Matrix_Completion/10000_trials/1}
    \caption[Convergence rates of the three algorithms over 10,000 trials.]{Convergence rates of the three algorithms over 10,000 trials. The figure shows the percentage of successful trials for each algorithm: AP, RRR, and RAAR. It can be observed that all three algorithms achieved high success rates, with one algorithm (RRR) demonstrating a slight advantage over the others. The parameters used were \( n = 20 \), \( r = 3 \), \( q = 50 \), the maximum number of iterations was 1000, \( \beta = 0.5 \), and a tolerance of \( 10^{-6} \).}
    \label{fig:success_percentage}
\end{figure}


In addition to evaluating the overall success percentage, we also analyzed the number of iterations required for convergence in each trial. Figure~\ref{fig:iteration_analysis} provides a detailed view of this analysis. The first figure provides an overview of the iteration counts for all trials where convergence occurred, while the second figure zooms in to highlight specific trials and their trends.
From these figures, an important observation emerges: while the RRR algorithm achieved the highest overall convergence percentage, it does not necessarily converge the fastest. In cases where other algorithms also converged, they often did so in fewer iterations compared to RRR. This trade-off highlights a subtle difference in the behavior of these algorithms. For example, in the zoomed-in figure, it is evident that for a specific trial (e.g., the trial indexed as 7), no algorithm other than RRR managed to converge, further underscoring its robustness. However, the iteration counts in trials where convergence was achieved by multiple algorithms reveal that AP and RAAR tend to require fewer steps, on average, to reach a solution.
It is important to note that cases in which an algorithm failed to converge are assigned the maximum number of iterations and are included in these figures. Therefore, the percentage-based success results from Figure~\ref{fig:success_percentage} remain crucial for interpreting the overall effectiveness of the algorithms.
\begin{figure}[h!]
    \centering
    % Subfigure (a)
    \begin{subfigure}[t]{0.8\textwidth} % Ensure fixed width for alignment
        \centering
        \includegraphics[width=\textwidth]{Matrix_Completion/10000_trials/iteration_counts}
        \caption{Iterations required for convergence for each algorithm.}
        \label{fig:iteration_counts}
    \end{subfigure}
    \hspace{0.02\textwidth} % Add small horizontal space between the subfigures
    % Subfigure (b)
    \begin{subfigure}[t]{0.8\textwidth} % Ensure fixed width for alignment
        \centering
        \includegraphics[width=\textwidth]{Matrix_Completion/10000_trials/iteration_zoom}
        \caption{Zoomed-in view highlighting specific trials, such as Trial 7, where only RRR converged.}
        \label{fig:iteration_zoom}
    \end{subfigure}
    
    % Main caption
    \caption[Overview of iteration counts for all trials where convergence occurred.]{(a) Iterations required for convergence for each algorithm. (b) Zoomed-in view highlighting specific trials, such as the trial indexed as 7, where only RRR converged. Appears in the figure the number of iterations to convergence (i.e., to satisfy the criterion), or the maximum value if the criterion was not met within the maximum number of iterations.}
    \label{fig:iteration_analysis}
\end{figure}
\noindent These findings emphasize the nuanced trade-offs between reliability and speed of convergence. While RRR remains the most robust algorithm, situations where fewer iterations are preferred might benefit from using AP or RAAR, provided they meet the convergence requirements.
Overall, these findings illustrate that while all three algorithms are capable of addressing matrix completion problems, the choice of algorithm can be informed by the specific requirements of the task, such as computational efficiency or the need for maximal reliability. Future work could explore larger matrices, different rank configurations, or various levels of missing data to deepen our understanding of these algorithms' performance and scalability.

\subsection{Effect of Increasing Missing Elements}

Another important factor analyzed in this study is the effect of increasing the number of missing elements on the convergence behavior of the algorithms. In this experiment, we incrementally increase the number of missing elements $q$ from $1$ up to $(n - r)^2$, which represents the theoretical limit for matrix recovery as described in Claim~\eqref{eq:deletable_elements}. To ensure consistency across all trials, the maximum number of iterations was set to $100,000$ for each algorithm. Two scenarios are considered:

\begin{itemize}
    \item \textbf{Case 1:} $n = 12$, $r = 3$
    \item \textbf{Case 2:} $n = 20$, $r = 5$
\end{itemize}
The results of these experiments are shown in Figure~\ref{fig:missing_elements_effect}. The first subplot represents the results for $n=12$ and $r=3$, while the second subplot represents $n=20$ and $r=5$. Each figure shows the number of iterations required for convergence as $q$ increases, but only for trials where the algorithms successfully converged. 


\begin{figure}[h!]
    \centering
    % Subfigure (a)
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Matrix_Completion/Increasing_Missing_Elements/n_12__r_3.png}
        \caption{$n = 12$, $r = 3$}
        \label{fig:missing_elements_a}
    \end{subfigure}
    \vspace{0.5cm}
    % Subfigure (b)
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Matrix_Completion/Increasing_Missing_Elements/n_20__r_5.png}
        \caption{$n = 20$, $r = 5$}
        \label{fig:missing_elements_b}
    \end{subfigure}
    \caption[Effect of increasing $q$ (number of missing elements) on convergence.]{Effect of increasing $q$ (number of missing elements) on convergence. (a) $n=12$, $r=3$. (b) $n=20$, $r=5$. Only trials where convergence occurred are shown. As $q$ approaches the theoretical limit $(n - r)^2$, RRR converges in more cases than the other algorithms. Trials reaching the maximum iteration limit of 100,000 are excluded, resulting in sparser points for algorithms that fail more often, which corresponds to greater spacing between points for those algorithms.}

    \label{fig:missing_elements_effect}
\end{figure}
From the figure, we observe a general upward trend in iteration counts for all algorithms, consistent with the expectation that increasing $q$ makes the problem more challenging. However, there is no dramatic difference in iteration counts among the algorithms in cases where they all converge. This indicates that AP, RRR, and RAAR exhibit similar iteration growth patterns as the percentage of missing elements increases.
A critical observation is that RRR successfully converges in significantly more cases than the other algorithms, especially as $q$ approaches the theoretical limit $(n - r)^2$. This is reflected in the density of points on the figure (as non-convergent cases are excluded): RRR shows a denser set of points, while the other algorithms exhibit sparser points, particularly at higher values of $q$, where they fail to converge in more trials. However, RRR does not necessarily converge faster in terms of iteration counts in trials where all algorithms succeed.
These results demonstrate that while all three algorithms show similar iteration trends for successful trials, RRR's robustness allows it to converge in cases where the others fail, particularly as the problem becomes more difficult with a higher number of missing elements. The sparser points for AP and RAAR at higher $q$ values reflect their inability to converge in many trials.

\chapter{Conclusion}

This thesis delved into the challenges and methodologies of phase retrieval and matrix completion, focusing on projection-based algorithms. By systematically evaluating their performance across various scenarios, this study sheds light on their strengths, limitations, and potential applications. Below is a synthesis of the key insights and their implications.

In scenarios involving random matrices, algorithms such as AP and RAAR demonstrated consistent performance with moderate success rates. These methods were reliable under standard conditions but struggled in more complex cases. In contrast, RRR and HIO algorithms exhibited superior adaptability and significantly higher convergence rates. Among these, RRR stood out for its efficiency, often requiring fewer iterations to converge than HIO, making it a robust choice for problems characterized by randomness.

For crystallographic phase retrieval problem, the algorithms' sensitivity to noise became apparent. As noise levels increased, convergence rates dropped, and the number of required iterations grew. Despite this, RRR once again proved its robustness, consistently achieving higher success rates than its counterparts. While AP and RAAR occasionally converged faster in low-noise environments, they were less reliable overall. This highlights the importance of robustness, particularly in noisy scenarios, where RRR excelled as a versatile and effective algorithm.

In matrix completion tasks, RRR emerged as the most reliable algorithm, excelling in scenarios with high levels of missing data. Its robustness enabled it to succeed where other methods failed, especially as the complexity of the problem increased. However, a nuanced trade-off was observed: although RRR was the most reliable, AP and RAAR often converged in fewer iterations when successful. As the percentage of missing elements increased, all algorithms required more iterations, but RRR consistently maintained its reliability, solidifying its role as the preferred choice in challenging conditions.

These findings underscore the importance of tailoring algorithm selection to specific problem requirements. For tasks demanding high reliability and adaptability, especially under challenging or noisy conditions, RRR is the most suitable choice. However, for problems where computational efficiency is a higher priority, AP or RAAR may provide faster results, provided convergence can be assured.

\subsection*{Future Directions} 

Building on these insights, future research could focus on enhancing algorithmic performance, particularly by developing hybrid methods that combine the strengths of multiple algorithms. For example, there are scenarios where simpler algorithms demonstrate faster convergence, making them more effective for specific problem instances. By integrating such algorithms with more sophisticated ones that may offer improved accuracy or robustness at later stages, it is possible to leverage the best of both worlds. This approach can optimize both efficiency and performance across diverse use cases.

Investigating scalability to larger, more complex problems and improving resilience to noise are essential for broadening the practical applicability of these techniques. Additionally, exploring their use in emerging applications, such as compressed sensing and advanced imaging methods, could provide valuable real-world validation.

This study highlights the trade-offs and considerations inherent in solving phase retrieval and matrix completion problems. By offering a clear understanding of the capabilities and limitations of existing algorithms, it establishes a foundation for theoretical advancements and practical applications, benefiting various fields reliant on signal and data recovery.

{
%\singlespacing
\backmatter
\bibliographystyle{unsrt}
\bibliography{references}
}
% \newpage
% \markboth{}{} 
% \thispagestyle{blankpagewithline}
% \mbox{}


% \newpage
% \begin{tikzpicture}[remember picture, overlay]
%   % Draw a horizontal line at the top of the page
%   \draw[thick] ([yshift=-1cm] current page.north west) -- ([yshift=-1cm] current page.north east);
% \end{tikzpicture}
% \includepdf[pages=-]{hebrew_intro.pdf}

% Insert PDF with a single centered top line
% \includepdf[page=1, pagecommand={%
%   \begin{tikzpicture}[remember picture, overlay]
%     % Draw a single horizontal line at the top of the page
%     \draw[thick] (current page.north west) -- (current page.north east);
%   \end{tikzpicture}
% }]{hebrew_intro.pdf}

\newpage
\thispagestyle{empty}
\mbox{}

\includepdf[pages=-]{hebrew_intro.pdf}


% \newpage
% \markboth{}{} 
% \thispagestyle{blankpagewithline}
% \mbox{}
\newpage
\thispagestyle{empty}
\mbox{}
\includepdf[pages=-]{hebrew_cover_in.pdf}
\newpage
\thispagestyle{empty}
\mbox{}
\includepdf[pages=-]{hebrew_cover_out.pdf}

\end{document}







